# Copyright 2024 BDP Ecosystem Limited. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
import functools

import brainstate as bst
import braintools as bts
import brainunit as u
import jax
import matplotlib.pyplot as plt
import numpy as np

from simple_dendrite_model_simulation import solve_explicit_solver

bst.environ.set(precision=64)


def visualize_a_simulate(params, f_current, show=True, title=''):
  saveat = u.math.arange(0., 100., 0.1) * u.ms
  ts, vs, _ = solve_explicit_solver(params, f_current, saveat)

  fig, gs = bts.visualize.get_figure(1, 1, 3.0, 4.0)
  ax = fig.add_subplot(gs[0, 0])
  plt.plot(ts.to_decimal(u.ms), u.math.squeeze(vs.to_decimal(u.mV)))
  plt.xlabel('Time [ms]')
  plt.ylabel('Potential [mV]')
  if title:
    plt.title(title)
  if show:
    plt.show()


def fitting_example():
  t1 = 200 * u.ms

  # Step 1: generating input currents
  saveat = u.math.arange(0., t1 / u.ms, 0.2) * u.ms

  def f_current(t, i_current, *args):
    return jax.lax.switch(
      i_current,
      [
        lambda t: u.math.where(t < 50. * u.ms,
                               0. * u.nA,
                               u.math.where(t < 100. * u.ms, 0.5 * u.nA, 0. * u.nA)),
        lambda t: u.math.where(t < 60. * u.ms,
                               0. * u.nA,
                               u.math.where(t < 160. * u.ms, 0.2 * u.nA, 0. * u.nA)),
        lambda t: u.math.where(t < 80. * u.ms,
                               0. * u.nA,
                               u.math.where(t < 160. * u.ms, 1.0 * u.nA, 0. * u.nA)),
        lambda t: u.math.where(t < 100. * u.ms,
                               0.2 * u.nA,
                               u.math.where(t < 150. * u.ms, 0.1 * u.nA, 0.3 * u.nA)),
      ],  # suppose there are 4 input currents
      t
    )

  # Step 2: generating the target neuronal parameters
  target_params = np.asarray([0.12, 0.036])

  # Step 3: generating the target potentials

  def simulate_per_param(param):
    indices = np.arange(4)
    fun = functools.partial(solve_explicit_solver, param, f_current, saveat, t1)
    _, simulated_vs, _ = jax.vmap(fun)((indices,))  # [n_input, T, n_compartment]
    return simulated_vs

  target_vs = simulate_per_param(target_params)

  # Step 4: initialize a batch of parameters to optimize,
  #         these parameters are candidates to be optimized
  bounds = [
    np.asarray([0.05, 0.01]),
    np.asarray([0.2, 0.1])
  ]
  n_batch = 8
  param_to_optimize = bst.ParamState(bst.random.uniform(bounds[0], bounds[1], (n_batch, bounds[0].size)))

  # Step 5: define the loss function and optimizers

  # calculate the loss for each parameter based on
  # the mismatch between the 4 simulated and target potentials
  def loss_per_param(param, step=10):
    simulated_vs = simulate_per_param(param)  # [n_input, T, n_compartment]
    losses = bts.metric.squared_error(simulated_vs.mantissa[..., ::step, 0], target_vs.mantissa[..., ::step, 0])
    return losses.mean()

  # calculate the average loss for all parameters,
  # this is the loss function to be gradient-based optimizations
  def loss_fun(step=10):
    return jax.vmap(functools.partial(loss_per_param, step=step))(param_to_optimize.value).mean()

  # find the best loss and parameter in the batch
  @bst.transform.jit
  def best_loss_and_param(params):
    losses = jax.vmap(loss_per_param)(params)
    i_best = u.math.argmin(losses)
    return losses[i_best], params[i_best]

  # define the optimizer
  optimizer = bst.optim.Adam(lr=1e-3)
  optimizer.register_trainable_weights({'param': param_to_optimize})

  # Step 6: training
  @bst.transform.jit
  def train_step_per_epoch():
    grads, loss = bst.transform.grad(loss_fun, grad_vars={'param': param_to_optimize}, return_value=True)()
    optimizer.update(grads)
    return loss

  for i_epoch in range(1000):
    loss = train_step_per_epoch()
    best_loss, best_param = best_loss_and_param(param_to_optimize.value)
    if best_loss < 1e-5:
      best_param = best_loss_and_param(param_to_optimize.value)[1]
      print(f'Epoch {i_epoch}, loss={loss}, best loss={best_loss}, best param={best_param}')
      break
    if i_epoch % 10 == 0:
      print(f'Epoch {i_epoch}, loss={loss}, best loss={best_loss}, best param={best_param}')

  # Step 7: visualize the results
  visualize_a_simulate(target_params, functools.partial(f_current, i_current=0), title='Target', show=False)
  visualize_a_simulate(best_param, functools.partial(f_current, i_current=0), title='Fitted', show=True)


if __name__ == '__main__':
  fitting_example()
